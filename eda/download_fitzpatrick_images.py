# This file is used to load the Fitzpatrick images into the bucket
# This file was generated by ChatGPT 4.1 :)

import pandas as pd
import requests
from google.cloud import storage
from PIL import Image
from io import BytesIO
import mimetypes
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

# --- CONFIGURE THESE VARIABLES ---
BUCKET_NAME = 'derma-datasets'
BUCKET_PATH_IMGS = 'raw/fitzpatrick17k/'
MAX_WORKERS = 8  # Adjust based on your machine/network (use nproc in terminal to determine number of CPUs; set this to 2-4 times that number)

def get_image_extension(image_bytes):
    try:
        img = Image.open(BytesIO(image_bytes))
        format = img.format.lower()
        if format == 'jpeg':
            return '.jpg'
        elif format == 'png':
            return '.png'
        elif format == 'gif':
            return '.gif'
        elif format == 'webp':
            return '.webp'
        elif format == 'bmp':
            return '.bmp'
        elif format == 'tiff':
            return '.tif'
        elif format == 'ico':
            return '.ico'
        else:
            return f".{format}"
    except Exception as e:
        print(f"Error reading image bytes: {e}")
        return None

def get_content_type(image_bytes, extension):
    try:
        img = Image.open(BytesIO(image_bytes))
        content_type = Image.MIME[img.format]
    except Exception:
        content_type, _ = mimetypes.guess_type("file" + extension)
        if not content_type:
            content_type = 'application/octet-stream'
    return content_type

def blob_md5_set(bucket):
    """
    Returns a set of md5hashes (with or without extension) from the files already in the bucket.
    If you might have various extensions, this checks for uniqueness only on the md5hash prefix.
    """
    md5_set = set()
    blobs = bucket.list_blobs()
    for blob in blobs:
        filename = blob.name
        if '.' in filename:
            md5 = filename.split('.')[0]
            md5_set.add(md5)
    return md5_set

def upload_one(row, md5_set, bucket):
    url = row['url']
    md5 = row['md5hash']

    if md5 in md5_set:
        return (url, 'skip', None)

    try:
        resp = requests.get(url, timeout=10)
        if resp.status_code != 200:
            return (url, 'fail', f"HTTP {resp.status_code}")
        image_bytes = resp.content
        extension = get_image_extension(image_bytes)
        if not extension:
            return (url, 'fail', "Could not determine extension")
        filename = md5 + extension
        content_type = get_content_type(image_bytes, extension)
        blob = bucket.blob(os.path.join(BUCKET_PATH_IMGS, filename))
        blob.upload_from_string(image_bytes, content_type=content_type)
        return (url, 'ok', None)
    except Exception as e:
        return (url, 'fail', str(e))

def main(df):
    # Get md5hashes already in the bucket
    print("Retrieving existing images from bucket...")
    md5_set = blob_md5_set(bucket)
    print(f"Found {len(md5_set)} unique md5hashes in bucket.")

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for row in df.to_dict(orient='records'):
            # Only schedule those not already uploaded
            if row['md5hash'] not in md5_set:
                futures.append(executor.submit(upload_one, row, md5_set, bucket))
            else:
                results.append((row['url'], 'skip', None))
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing images"):
            results.append(future.result())

    # Summarize results
    num_ok = sum(1 for r in results if r[1] == 'ok')
    num_skip = sum(1 for r in results if r[1] == 'skip')
    num_fail = sum(1 for r in results if r[1] == 'fail')
    print(f"Done!\nUploaded: {num_ok}\nAlready existed: {num_skip}\nFailed: {num_fail}")

    # Optional: Save failures for inspection
    fail_rows = [r for r in results if r[1] == 'fail']
    if fail_rows:
        df_fail = pd.DataFrame(fail_rows, columns=['url', 'status', 'reason'])
        df_fail.to_csv('fitzpatrick_upload_failures.csv', index=False)
        print(f"Failures written to upload_failures.csv")

if __name__ == '__main__':
    # Load Fitzpatrick csv
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    fitz_csv_blob = bucket.blob('raw/fitzpatrick17k/fitzpatrick17k.csv')
    fitz_csv_data = blob.download_as_text()
    fitz_csv = pd.read_csv(StringIO(fitz_csv_data))
    fitz_csv = fitz_csv.iloc[0,:]
    main(fitz_csv)
