# Model Training Configuration

# Data source configuration
data:
  use_local: True
  metadata_path: "/Users/tk20/Desktop/APCOMP215/data/metadata_all_harmonized.csv"  # Override default if needed
  img_prefix: "/Users/tk20/Desktop/APCOMP215/data/imgs/"  # Override default if needed
  min_samples_per_label: 1000
  datasets:
    - fitzpatrick17k
    # - ddi
    # - isic
  has_text: null # True to filter for only datasets with text descriptions; False to filter for datasets without text descriptions.
  
  # Data splits
  test_size: 0.2
  val_size: 0.1  # Set to 0.15 for train/val/test split
  
  # Image configuration
  img_size: [224, 224]  # [height, width]
  
  # Random seed for reproducibility
  seed: 42

# Training parameters
training:
  batch_size: 12
  num_workers: 0
  prefetch_factor: 1
  num_epochs: 100
  patience: 100
  validation_interval: 1  # Run validation every N epochs (1 = every epoch)
  n_warmup_epochs: 5  # Number of epochs to freeze backbone (0 = no freezing)
  
  # Statistics and sampling
  compute_stats: False
  weighted_sampling: False
  
  # Scheduler configuration
  scheduler:
    use_cosine_annealing: true  # Enable cosine annealing learning rate scheduler
    vision_eta_min: 0.0000001  # Minimum learning rate for backbone optimizer
    images_classifier_eta_min: 0.0000001  # Minimum learning rate for images head optimizer

# Augmentation parameters
# Set to null to disable a transform, or provide a value to enable it
augmentation:
  brightness_jitter: 0.25
  contrast_jitter: 0.25
  saturation_jitter: 0.25
  hue_jitter: 0.05
  rotation_degrees: 25
  translate: null
  scale: [0.8, 1.1]
  grayscale_prob: null
  horizontal_flip_prob: 0.5
  vertical_flip_prob: 0.2

# Vision model configuration
vision_model:
  name: "resnet50"  # resnet50, resnet101, densenet121, efficientnet_b0, vgg16, vit_b_16, vit_b_32, vit_l_16, vit_l_32
  pretrained: true
  pooling_type: "max"  # For ImageNet models: avg, max, concat
  unfreeze_layers: 20  # Number of layers to unfreeze from end

# Images classifier configuration
images_classifier:
  num_classes: null  # Will be inferred from data
  hidden_sizes: [12]  # List of hidden layer sizes
  activation: "relu"  # relu, gelu, swish, leaky_relu
  dropout_rate: 0.6
  label_smoothing: 0.1  # Label smoothing factor for cross entropy loss (0.0 = no smoothing)
  loss_fn: "cross_entropy"  # cross_entropy, focal

# Text classifier configuration
text_classifier:
  hidden_sizes: [128]  # List of hidden layer sizes for text head
  activation: "relu"   # Same options as image classifier
  dropout_rate: 0.2
  loss_fn: "cross_entropy"  # Same options as image classifier

# Fusion configuration
fusion:
  strategy: "weighted_sum"  # Only weighted_sum for now
  alpha_image: 0.6  # Weight for image logits (0.0 to 1.0), text weight = 1 - alpha_image
  # Note: alpha_image automatically determines text weight, so they always sum to 1.0

# Encoder configuration
# IMPORTANT: If you change the model_name, max_length, pooling_type, etc. you must also change the 
# pre_existing_path. Otherwise, the dataloader will just load the existing embeddings (computed with the previous model), 
# instead of re-computing them with the new model. To make the configuration recompute the embeddings each time, set
# pre_existing_path to null. This will not save the embeddings and will force a recompute from scratch.
encoder:
  pre_existing_path: null # The path to which to store the embeddings (to store in Google cloud, begin path with gs://<bucket-name>/). Set to null to not save embeddings anywhere.
  model_name: "pubmedbert" # or biosyn, sapbert, qwen, or path to a model
  max_length: 512 # Maximum sequence length
  pooling_type: "cls" # One of 'mean', 'cls', 'last_token'
  batch_size: 32 # The batch size to use when creating embeddings
  qwen_instr: "Represent the patient's description and characteristics (including clinical details, age, sex, and symptoms) as a semantic embedding suitable for clinical classification. Return an embedding." # The instructions for the QWEN embedding to use (allows us to experiment with different instructions; ignored if model is not QWEN)

# Optimizer configuration
optimizer:
  vision_model:
    name: "adamw"
    learning_rate: 0.00007
    weight_decay: 0.0001
    momentum: 0.9
    betas: [0.9, 0.999]
    eps: 0.00000001
  
  images_classifier:
    name: "adamw"
    learning_rate: 0.0007
    weight_decay: 0.0001
    momentum: 0.9
    betas: [0.9, 0.999]
    eps: 0.00000001
  
  text_classifier:
    name: "adamw"
    learning_rate: 0.0007
    weight_decay: 0.0001
    momentum: 0.9
    betas: [0.9, 0.999]
    eps: 0.00000001

# Output configuration
output:
  save_dir: "./checkpoints"
  log_dir: "./logs"
  experiment_name: "skin_disease_classification"
  wandb_project: "APCOMP215"

# Checkpoint configuration
checkpoint:
  save_frequency: 1000  # Save checkpoint every N epochs (it saves the best model anyway)
  keep_last: 1  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to load from (null for no loading)
