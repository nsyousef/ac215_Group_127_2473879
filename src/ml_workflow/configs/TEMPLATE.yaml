# Model Training Configuration

# Data source configuration
data:
  use_local: False
  metadata_path: "../../data/metadata_all_harmonized.csv"  # Override default if needed
  img_prefix: "../../data"  # Override default if needed
  min_samples_per_label: 10
  datasets:
    - fitzpatrick17k
#    - ddi
#    - isic

# Training parameters
training:
  batch_size: 16
  num_workers: 2
  seed: 42
  prefetch_factor: 1
  num_epochs: 100
  patience: 10
  validation_interval: 1  # Run validation every N epochs (1 = every epoch)

# Data splits
splits:
  test_size: 0.2
  val_size: 0.1  # Set to 0.15 for train/val/test split

# Image configuration
image:
  size: [224, 224]  # [height, width]
  mode: "RGB"

# Augmentation parameters
# Set to null to disable a transform, or provide a value to enable it
augmentation:
  brightness_jitter: 0.1
  contrast_jitter: 0.1
  saturation_jitter: 0.1
  hue_jitter: 0.1
  rotation_degrees: 15
  translate: null
  scale: [0.9, 1.1]
  grayscale_prob: null
  horizontal_flip_prob: 0.5
  vertical_flip_prob: 0.5

# Statistics and sampling
data_processing:
  compute_stats: False
  weighted_sampling: True
  skip_errors: true

# Model configuration
model:
  name: "densenet121"  # resnet50, resnet101, densenet121, efficientnet_b0, vgg16
  pretrained: true
  num_classes: null  # Will be inferred from data
  hidden_sizes: [512, 256]  # List of hidden layer sizes
  activation: "relu"  # relu, gelu, swish, leaky_relu
  dropout_rate: 0.2

# Separate optimizers (override if provided). If omitted, falls back to single optimizer above
optimizer_backbone:
  name: "adamw"
  learning_rate: 0.0001
  weight_decay: 0.00001
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 0.00000001

optimizer_head:
  name: "adamw"
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 0.00000001

# Output configuration
output:
  save_dir: "./checkpoints"
  log_dir: "./logs"
  experiment_name: "skin_disease_classification"

# Checkpoint configuration
checkpoint:
  save_frequency: 1000  # Save checkpoint every N epochs
  keep_last: 1  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to load from (null for no loading)
