# Model Training Configuration for Modal
# EfficientNet-B0 + SapBERT 512 (CLS pooling) Experiment

# Data source configuration  
data:
  use_local: True  # Use Modal Volume for fast local SSD access
  metadata_path: "/data/dataset_v1/metadata_all_harmonized.csv"  # From Modal Volume
  img_prefix: "/data/dataset_v1/imgs"  # From Modal Volume
  min_samples_per_label: 100
  datasets:
    - fitzpatrick17k
    - ddi
    - isic
    - derm1m
  derm1m_sources: # Only applicable if derm1m is being used under "datasets"
    - IIYI 
    - edu
    - note
    - public
    - pubmed 
    - reddit
    - twitter
    - youtube
  has_text: True # True to filter for only datasets with text descriptions; False to filter for datasets without text descriptions.

  # Data splits
  test_size: 0.1
  val_size: 0.1
  
  # Image configuration
  img_size: [224, 224]  # [height, width]
  
  # Random seed for reproducibility
  seed: 42

# Training parameters
training:
  batch_size: 256
  num_workers: 16
  prefetch_factor: 8
  num_epochs: 10
  patience: 5
  validation_interval: 1  # Run validation every N epochs (1 = every epoch)
  n_warmup_epochs: 3  # Number of epochs to freeze backbone (0 = no freezing)
  
  # Statistics and sampling
  compute_stats: False
  weighted_sampling: False
  
  # Scheduler configuration
  scheduler:
    use_cosine_annealing: False  # Enable cosine annealing learning rate scheduler
    vision_eta_min: 0.0000001  # Minimum learning rate for backbone optimizer
    multimodal_classifier_eta_min: 0.0000001  # Minimum learning rate for images head optimizer

# Augmentation parameters
# Set to null to disable a transform, or provide a value to enable it
augmentation:
  brightness_jitter: 0.1
  contrast_jitter: 0.1
  saturation_jitter: 0.1
  hue_jitter: 0.01
  rotation_degrees: 25
  translate: null
  scale: [0.9, 1.1]
  grayscale_prob: null
  horizontal_flip_prob: 0.5
  vertical_flip_prob: null

# Vision model configuration
vision_model:
  name: "efficientnet_b0"  # resnet50, resnet101, densenet121, efficientnet_b0, efficientnet_b1, vgg16, vit_b_16, vit_b_32, vit_l_16, vit_l_32
  pretrained: true
  pooling_type: "avg"  # For ImageNet models: avg, max, concat
  unfreeze_layers: 10  # Number of layers to unfreeze from end

# Multimodal classifier configuration
multimodal_classifier:
  # Projection configuration
  projection_dim: 256  # Common projection dimension for both modalities
  use_l2_normalization: false  # Normalize embeddings before fusion

  # Vision projection layers (empty list = just linear layer)
  image_projection_hidden: [512]  # Hidden layers for vision projection
  projection_activation: "relu"  # Activation for projection layers
  projection_dropout: 0.3  # Dropout for projection layers
  
  # Text projection layers (empty list = just linear layer)  
  text_projection_hidden: [512]  # Hidden layers for text projection
  
  # Final classifier configuration
  final_hidden_sizes: [256, 128]  # Hidden layers after fusion
  final_activation: "relu"  # Activation for final classifier
  final_dropout: 0.3  # Dropout for final classifier
  
  # Fusion strategy
  fusion_strategy: "concat_mlp"  # "weighted_sum" or "concat_mlp"
  
  # Auxiliary losses (optional)
  use_auxiliary_loss: False  # Enable auxiliary losses on individual modalities
  auxiliary_loss_weight: 0.0  # Weight for auxiliary losses (0.0-1.0)
  
  # Loss function configuration
  loss_fn: "cross_entropy"  # "cross_entropy" or "focal"
  label_smoothing: 0.1  # For cross_entropy loss
  use_class_weights_from_data: false  # Automatically inject class weights from dataloader
  
  # Focal loss parameters (only used if loss_fn is "focal")
  # alpha: 1.0  # Weighting factor for rare class (or sample_weights will be used)
  # gamma: 2.0  # Focusing parameter
  # reduction: "mean"  # Reduction method

# Modality masking configuration
# Used to train models that can handle missing modalities (modality dropout)
masking:
  # Complete masking: mask entire modality for the whole run
  # Options: null (no complete masking), "image" (mask all images), "text" (mask all text)
  mask_complete: null  # null, "image", or "text"
  
  # Random masking during training (modality dropout)
  # Only applies if mask_complete is null
  random_mask:
    enabled: false  # Enable random masking
    image_prob: 0.0  # Probability of masking each image (0.0-1.0)
    text_prob: 0.0  # Probability of masking each text embedding (0.0-1.0)

# Encoder configuration
encoder:
  pre_existing_path: "/data/dataset_v1/embeddings/sapbert_512_cls.parquet" # From Modal Volume (local SSD)
  model_name: "sapbert" # or biosyn, sapbert, qwen, or path to a model
  max_length: 512 # Maximum sequence length
  pooling_type: "cls" # One of 'mean', 'cls', 'last_token'
  batch_size: 32 # The batch size to use when creating embeddings
  qwen_instr: "Represent the patient's description and characteristics (including clinical details, age, sex, and symptoms) as a semantic embedding suitable for clinical classification. Return an embedding." # The instructions for the QWEN embedding to use (allows us to experiment with different instructions; ignored if model is not QWEN)

# Optimizer configuration
optimizer:
  vision_model:
    name: "adamw"
    learning_rate: 0.0001
    weight_decay: 0.001
    betas: [0.9, 0.999]
    eps: 0.00000001
  
  multimodal_classifier:
    name: "adamw"
    learning_rate: 0.0005
    weight_decay: 0.001
    betas: [0.9, 0.999]
    eps: 0.00000001

# Output configuration
output:
  save_dir: "/checkpoints"
  log_dir: "./logs"
  experiment_name: "exp_arch_006_efficientnet_b0_sapbert512_cls"
  wandb_project: "APCOMP215"

# Checkpoint configuration
checkpoint:
  save_frequency: 10  # Save checkpoint every N epochs (it saves the best model anyway)
  keep_last: 1  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to load from (null for no loading)

