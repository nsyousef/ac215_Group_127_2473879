# Model Training Configuration for Modal
# Updated to match recent experiment best practices

# Data source configuration
data:
  use_local: True  # Use Modal Volume for fast local SSD access
  dataset: dataset_v2  # Dataset name (auto-constructs metadata_path and img_prefix)
  min_samples_per_label: 250  # Minimum images per class
  datasets:
    - fitzpatrick17k
    - ddi
    - isic
    - derm1m
  # Per-dataset split ratios (optional - overrides test_size/val_size if provided)
  per_dataset_split_ratios:
    fitzpatrick17k:
      train: 0.9
      val: 0.1
      test: 0.0
    ddi:
      train: 0.9
      val: 0.1
      test: 0.0
    isic:
      train: 0.9
      val: 0.1
      test: 0.0
    derm1m:
      train: 0.9
      val: 0.1
      test: 0.0
  derm1m_sources: # Only applicable if derm1m is being used under "datasets"
    - IIYI
    - edu
    - note
    - public
    - pubmed
    - reddit
    - twitter
    - youtube
  has_text: null # True to filter for only datasets with text descriptions; False to filter for datasets without text descriptions.

  # Data splits (used if per_dataset_split_ratios not provided)
  test_size: 0.2
  val_size: 0.1

  # Image configuration
  img_size: [380, 380]  # [height, width] - 380x380 for EfficientNet-B4

  # Random seed for reproducibility
  seed: 42

# Training parameters
training:
  batch_size: 64  # Adjust based on GPU memory
  num_workers: 16  # Number of data loading workers
  prefetch_factor: 8  # Batches to prefetch per worker
  num_epochs: 100
  patience: 20  # Early stopping patience
  validation_interval: 1  # Run validation every N epochs (1 = every epoch)
  n_warmup_epochs: 5  # Number of epochs to freeze backbone (0 = no freezing)

  # Statistics and sampling
  compute_stats: False
  use_normalization: True  # Use dataset normalization (recommended)
  weighted_sampling: False  # Balance class frequencies

  # Vision-only pre-training (optional)
  vision_only_pretraining:
    enabled: false  # Set to true to pre-train vision model before multimodal training
    epochs: 0  # Number of epochs for vision-only pre-training

  # Auxiliary loss scheduling (replaces old auxiliary_loss_weight)
  auxiliary_loss_schedule:
    vision:
      schedule_type: "constant"  # "constant" or "linear"
      start_epoch: 0
      end_epoch: 100
      start_weight: 0.5  # Auxiliary vision loss weight
      end_weight: 0.5
    text:
      schedule_type: "constant"
      start_epoch: 0
      end_epoch: 100
      start_weight: 0.3  # Auxiliary text loss weight
      end_weight: 0.3

  # Scheduler configuration
  scheduler:
    use_cosine_annealing: true  # Enable cosine annealing learning rate scheduler
    vision_eta_min: 0.000001  # Minimum learning rate for backbone optimizer
    multimodal_classifier_eta_min: 0.000001  # Minimum learning rate for classifier optimizer

# Augmentation parameters
# Set to null to disable a transform, or provide a value to enable it
augmentation:
  brightness_jitter: 0.3
  contrast_jitter: 0.3
  saturation_jitter: 0.3
  hue_jitter: 0.05
  rotation_degrees: 20
  translate: [0.1, 0.1]  # [x, y] translation range
  scale: [0.9, 1.1]
  grayscale_prob: 0.1  # Probability of converting to grayscale
  horizontal_flip_prob: 0.5
  vertical_flip_prob: 0.3

# Vision model configuration
vision_model:
  name: "efficientnet_b4"  # resnet50, resnet101, densenet121, efficientnet_b0/b1/b4, vit_b_16, vit_b_32, vit_l_16, vit_l_32
  pretrained: true
  pooling_type: "concat"  # For ImageNet models: avg, max, concat (concat recommended for EfficientNet)
  unfreeze_layers: -1  # Number of layers to unfreeze from end (-1 = unfreeze all)

# Multimodal classifier configuration
multimodal_classifier:
  # These will be set automatically from data and models:
  num_classes: null  # Will be inferred from data
  sample_weights: null  # Will be inferred from data
  vision_embedding_dim: null  # Will be inferred from vision model
  text_embedding_dim: null  # Will be inferred from text embeddings

  # Projection configuration
  projection_dim: 256  # Common projection dimension for both modalities
  use_l2_normalization: true  # Normalize embeddings before fusion

  # Vision projection layers (empty list = just linear layer)
  image_projection_hidden: [512, 256]  # Hidden layers for vision projection
  projection_activation: "gelu"  # Activation for projection layers (relu or gelu)
  projection_dropout: 0.6  # Dropout for projection layers

  # Text projection layers (empty list = just linear layer)
  text_projection_hidden: [512]  # Hidden layers for text projection
  text_projection_dropout: 0.5  # Dropout for text projection (optional, separate from projection_dropout)

  # Final classifier configuration
  final_hidden_sizes: [128]  # Hidden layers after fusion
  final_activation: "gelu"  # Activation for final classifier (relu or gelu)
  final_dropout: 0.3  # Dropout for final classifier

  # Fusion strategy
  fusion_strategy: "concat_mlp"  # "weighted_sum" or "concat_mlp" (concat_mlp recommended)

  # Auxiliary losses
  use_auxiliary_loss: true  # Enable auxiliary losses on individual modalities
  # Note: Use auxiliary_loss_schedule in training section instead of auxiliary_loss_weight

  # Loss function configuration
  loss_fn: "cross_entropy"  # "cross_entropy" or "focal"
  label_smoothing: 0.1  # For cross_entropy loss
  use_class_weights_from_data: true  # Automatically use class weights from data

  # Focal loss parameters (only used if loss_fn is "focal")
  # gamma: 1.5  # Focusing parameter
  # alpha: null  # Weighting factor (null = use class weights from data)

# Modality masking configuration
masking:
  mask_complete: null  # null (no complete masking), "image" (mask all images), or "text" (mask all text)

  random_mask:
    enabled: True  # Enable random masking during training (modality dropout)
    image_prob: 0.0  # Probability of masking each image (0.0-1.0)
    text_prob: 0.1  # Probability of masking each text embedding (0.0-1.0)

  epoch_schedule:
    enabled: false  # Enable epoch-based masking schedule

# Encoder configuration
# IMPORTANT: If you change the model_name, max_length, pooling_type, etc. you must also change the
# embedding_filename. Otherwise, the dataloader will just load the existing embeddings (computed with the previous model),
# instead of re-computing them with the new model. To make the configuration recompute the embeddings each time, set
# embedding_filename to null. This will not save the embeddings and will force a recompute from scratch.
encoder:
  embedding_filename: "pubmedbert_256_cls.parquet"  # Filename of pre-computed embeddings. Full path is auto-constructed from dataset. Set to null to not use pre-computed embeddings.
  model_name: "pubmedbert"  # or biosyn, sapbert, qwen, or path to a model
  max_length: 256  # Maximum sequence length (256 recommended for recent experiments)
  pooling_type: "cls"  # One of 'mean', 'cls', 'last_token'
  batch_size: 32  # The batch size to use when creating embeddings
  qwen_instr: ""  # Instructions for QWEN models (ignored if model is not QWEN)

# Optimizer configuration
optimizer:
  vision_model:
    name: "adamw"
    learning_rate: 0.0001
    weight_decay: 0.001
    betas: [0.9, 0.999]
    eps: 0.00000001

  multimodal_classifier:
    name: "adamw"
    learning_rate: 0.0005
    weight_decay: 0.0001
    betas: [0.9, 0.999]
    eps: 0.00000001

# Output configuration
output:
  save_dir: "/checkpoints"
  log_dir: "./logs"
  experiment_name: "modal_template"  # Fallback name if config_filename is not available
  wandb_project: "APCOMP215"

# Checkpoint configuration
checkpoint:
  save_frequency: 5  # Save checkpoint every N epochs (it saves the best model anyway)
  keep_last: 2  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to load from (null for no loading)
