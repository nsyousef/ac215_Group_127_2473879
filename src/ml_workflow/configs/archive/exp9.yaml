# Model Training Configuration

# Data source configuration
data:
  use_local: True
  metadata_path: "/orcd/data/omarabu/001/tanush20/derma/data/metadata_all_harmonized.csv"  # Override default if needed
  img_prefix: "/orcd/data/omarabu/001/tanush20/derma/data/imgs/"  # Override default if needed
  min_samples_per_label: 5
  datasets:
    - fitzpatrick17k
    - ddi
    - isic

# Training parameters
training:
  batch_size: 64
  num_workers: 20
  seed: 42
  prefetch_factor: 1
  num_epochs: 1000
  patience: 100
  validation_interval: 1  # Run validation every N epochs (1 = every epoch)
  n_warmup_epochs: 5  # Number of epochs to freeze backbone (0 = no freezing)

  # Scheduler configuration
  scheduler:
    use_cosine_annealing: true  # Enable cosine annealing learning rate scheduler
    backbone_eta_min: 0.0000001  # Minimum learning rate for backbone optimizer
    head_eta_min: 0.0000001  # Minimum learning rate for head optimizer

# Data splits
splits:
  test_size: 0.2
  val_size: 0.1  # Set to 0.15 for train/val/test split

# Image configuration
image:
  size: [448, 448]  # [height, width]
  mode: "RGB"

# Augmentation parameters
# Set to null to disable a transform, or provide a value to enable it
augmentation:
  brightness_jitter: 0.25
  contrast_jitter: 0.25
  saturation_jitter: 0.25
  hue_jitter: 0.05
  rotation_degrees: 25
  translate: null
  scale: [0.8, 1.1]
  grayscale_prob: null
  horizontal_flip_prob: 0.5
  vertical_flip_prob: 0.2

# Statistics and sampling
data_processing:
  compute_stats: False
  weighted_sampling: False
  skip_errors: true

# Model configuration
model:
  name: "resnet101"  # resnet50, resnet101, densenet121, efficientnet_b0, vgg16
  pretrained: true
  num_classes: null  # Will be inferred from data
  hidden_sizes: [512, 64]  # List of hidden layer sizes
  activation: "relu"  # relu, gelu, swish, leaky_relu
  dropout_rate: 0.5
  #label_smoothing: 0.15  # Label smoothing factor for cross entropy loss (0.0 = no smoothing)
  loss_fn: "focal"
  alpha: 1.0
  gamma: 2.0
  reduction: "mean"


# Separate optimizers (override if provided). If omitted, falls back to single optimizer above
optimizer_backbone:
  name: "adamw"
  learning_rate: 0.00007
  weight_decay: 0.0001
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 0.00000001

optimizer_head:
  name: "adamw"
  learning_rate: 0.0007
  weight_decay: 0.0001
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 0.00000001

# Output configuration
output:
  save_dir: "./checkpoints"
  log_dir: "./logs"
  experiment_name: "skin_disease_classification"

# Checkpoint configuration
checkpoint:
  save_frequency: 1000  # Save checkpoint every N epochs (it saves the best model anyway)
  keep_last: 1  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to load from (null for no loading)
