# ML Workflow for Skin Disease Classification

A configurable machine learning pipeline for skin disease classification built for APCOMP215.

## üìÅ Directory Structure

```
ml-workflow/
‚îú‚îÄ‚îÄ main.py                    # Main entry point for training
‚îú‚îÄ‚îÄ eval.py                    # Model evaluation script
‚îú‚îÄ‚îÄ constants.py               # Project constants
‚îú‚îÄ‚îÄ utils.py                   # Utilities, checkpointing & data analysis
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ TEMPLATE.yaml          # Configuration template
‚îú‚îÄ‚îÄ dataloader/
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py          # ImageDataset & create_dataloaders
‚îÇ   ‚îî‚îÄ‚îÄ transform_utils.py     # Image transformations
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ imagenet.py            # Transfer learning model
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Model utilities (MLP)
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ train.py               # Training class
‚îî‚îÄ‚îÄ requirements.txt           # Dependencies
```

## Quick Start

### 1. Training a Model

Set your parameters in the config file and run:

```bash
python main.py --config path/to/your/config.yaml
```

### 2. Evaluating a Model

Enter the checkpoint path in the config and run:

```bash
python eval.py --config configs/config.yaml --plot-dir ./my_plots
```

### 3. Using in Jupyter Notebooks

```python
from main import initialize_model

# Initialize model and data
return_dict = initialize_model('configs/config.yaml')
trainer = return_dict['trainer']
model = return_dict['vision_model']
test_loader = return_dict['test_loader']

# Train the model or evaluate as required
trainer.train()
```

## ‚öôÔ∏è Configuration

The workflow is fully configurable via YAML files in the `configs/` directory.

### Data Source Configuration

| Parameter | Description |
|-----------|-------------|
| `use_local` | Whether to load data from local storage instead of cloud or remote sources |
| `metadata_path` | Path to the metadata CSV that defines image paths, labels, and text descriptions (only used if `use_local` is True) |
| `img_prefix` | Directory for image files |
| `min_samples_per_label` | Minimum number of images required for a label to be included |
| `datasets` | List of datasets to include (e.g., Fitzpatrick17k, DDI, ISIC) |
| `has_text` | If `True`, only include data with text descriptions; if `False`, only include data without text descriptions; if `None`, do not filter on presence of text descriptions. |

### Training Parameters

| Parameter | Description |
|-----------|-------------|
| `batch_size` | Number of samples per batch |
| `num_workers` | Number of CPU threads used for data loading |
| `seed` | Random seed for reproducibility |
| `prefetch_factor` | Controls how many batches each worker preloads |
| `num_epochs` | Maximum number of training epochs |
| `patience` | Number of epochs to wait before early stopping if validation doesn't improve |
| `validation_interval` | Frequency (in epochs) to run validation |
| `n_warmup_epochs` | Number of epochs to freeze the backbone |

#### Scheduler

| Parameter | Description |
|-----------|-------------|
| `use_cosine_annealing` | Enables cosine annealing learning rate scheduler |
| `backbone_eta_min` | Minimum learning rate for backbone during cosine annealing |
| `head_eta_min` | Minimum learning rate for classification head during cosine annealing |

### Data Splits

| Parameter | Description |
|-----------|-------------|
| `test_size` | Fraction of data reserved for testing |
| `val_size` | Fraction of data reserved for validation |

### Image Configuration

| Parameter | Description |
|-----------|-------------|
| `size` | Image dimensions `[height, width]` |
| `mode` | Image color mode (e.g., RGB or grayscale) |

### Augmentation Parameters

| Parameter | Description |
|-----------|-------------|
| `brightness_jitter` | Random brightness perturbations for robustness (set to `null` to disable) |
| `contrast_jitter` | Random contrast perturbations |
| `saturation_jitter` | Random saturation perturbations |
| `hue_jitter` | Random hue perturbations |
| `rotation_degrees` | Random rotation range in degrees |
| `scale` | Random scaling range (e.g., `[0.8, 1.1]`) |
| `horizontal_flip_prob` | Probability of horizontal flipping |
| `vertical_flip_prob` | Probability of vertical flipping |
| `translate` | Translation range |
| `grayscale_prob` | Probability of converting to grayscale |

### Data Processing

| Parameter | Description |
|-----------|-------------|
| `compute_stats` | Whether to recompute dataset statistics (mean, std). If false, uses precomputed ImageNet statistics |
| `weighted_sampling` | Balances image sampling frequency inversely to class counts for class imbalance correction |

### Model Configuration

| Parameter | Description |
|-----------|-------------|
| `name` | Model backbone (e.g., "resnet101", "densenet121", "resnet50") |
| `pretrained` | Whether to load pretrained weights |
| `num_classes` | Number of output classes (set to `null` for auto-inference based on data) |
| `hidden_sizes` | Sizes of hidden layers in classification head |
| `activation` | Non-linear activation function (e.g., "relu") |
| `dropout_rate` | Fraction of neurons dropped during training for regularization |
| `label_smoothing` | Smooths target labels to reduce overconfidence (used with cross entropy loss) |
| `loss_fn` | Type of loss function (e.g., "cross_entropy", "focal") |
| `pooling_type` | Type of feature pooling (e.g., "max" or "sum") |
| `unfreeze_layers` | Number of final backbone layers to unfreeze during fine-tuning |

### Optimizer Configuration

Separate configurations can be set for backbone and head optimizers:

| Parameter | Description |
|-----------|-------------|
| `name` | Optimizer algorithm (e.g., "adam", "adamw") |
| `learning_rate` | Initial learning rate (separate values can be used for backbone and head) |
| `weight_decay` | L2 regularization coefficient |
| `momentum` | Momentum term (used in SGD) |
| `betas` | Exponential decay rates for first and second moment estimates (Adam-family) |
| `eps` | Small constant added for numerical stability |

### Output and Checkpointing

| Parameter | Description |
|-----------|-------------|
| `save_dir` | Directory to save model checkpoints |
| `log_dir` | Directory to store logs and metrics |
| `experiment_name` | Label for the experiment (used for naming files) |
| `save_frequency` | Frequency (in epochs) for saving checkpoints |
| `keep_last` | Number of most recent checkpoints to keep |
| `load_from` | Path to a pretrained checkpoint to resume from |

### Example Checkpoint Configuration

The model automatically keeps the checkpoint with the best validation accuracy. You can continue training from a checkpoint by specifying the path:

```yaml
checkpoint:
  save_frequency: 5    # Save every 5 epochs
  keep_last: 10        # Keep last 10 checkpoints
  load_from: "./checkpoints/best_model.pth"  # Resume from checkpoint
```

## üìã Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```
