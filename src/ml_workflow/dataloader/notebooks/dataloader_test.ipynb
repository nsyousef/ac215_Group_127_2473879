{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a64551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import modular components\n",
    "import os\n",
    "# Fix tokenizers parallelism warning when using multiprocessing DataLoaders\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add src directory to path for absolute imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Go up 2 levels: notebooks/ -> dataloader/ -> ml_workflow/ -> src/\n",
    "src_path = Path().absolute().parent.parent.parent\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from ml_workflow.constants import (GCS_BUCKET_NAME, GCS_METADATA_PATH, GCS_IMAGE_PREFIX, \n",
    "                                   LOCAL_METADATA_PATH, LOCAL_DATA_DIR, IMG_ID_COL)\n",
    "from ml_workflow.utils import load_metadata, logger, stratified_split\n",
    "from ml_workflow.dataloader import (create_dataloaders, ImageDataset, \n",
    "                                    get_basic_transform, get_train_transform, get_test_valid_transform)\n",
    "from ml_workflow.dataloader.embedding_utils import load_or_compute_embeddings\n",
    "import torch\n",
    "\n",
    "# Set logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "print(\"✓ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66b2bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:57:03,483 - ml_workflow.utils - INFO - After sampling: 1,918 samples (1.0% of 191,523 total)\n",
      "2025-11-11 15:57:03,485 - ml_workflow.utils - INFO - Filtered out 32 classes with < 5 samples\n",
      "2025-11-11 15:57:03,485 - ml_workflow.utils - INFO - Final dataset: 1,849 samples across 49 classes\n",
      "2025-11-11 15:57:03,486 - ml_workflow.utils - INFO - Computing/loading embeddings on device: mps\n",
      "2025-11-11 15:57:03,487 - ml_workflow.utils - WARNING - File at path [embeddings_test_small.parquet] does not exist. Computing embeddings from scratch.\n",
      "2025-11-11 15:57:25,071 - ml_workflow.utils - INFO - Embeddings merged. Metadata shape: (1849, 23)\n",
      "2025-11-11 15:57:25,072 - ml_workflow.utils - INFO - Creating DataLoaders\n",
      "2025-11-11 15:57:25,075 - ml_workflow.utils - INFO - Train samples: 1,479, Test samples: 370\n",
      "2025-11-11 15:57:25,077 - ml_workflow.utils - INFO - Total classes: 49\n",
      "2025-11-11 15:57:25,077 - ml_workflow.utils - INFO - Computing dataset statistics from all training data\n",
      "2025-11-11 15:57:25,105 - ml_workflow.utils - INFO - Computing dataset statistics...\n",
      "Computing stats: 100%|██████████| 47/47 [01:07<00:00,  1.43s/it]\n",
      "2025-11-11 15:58:32,134 - ml_workflow.utils - INFO - Mean: [0.591986894607544, 0.4655754566192627, 0.425141304731369]\n",
      "2025-11-11 15:58:32,134 - ml_workflow.utils - INFO - Std: [0.17733310163021088, 0.1639116406440735, 0.1614614576101303]\n",
      "2025-11-11 15:58:32,181 - ml_workflow.utils - INFO - Using weighted sampling for imbalanced data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GCS DataLoader (sampled) Created!\n",
      "==================================================\n",
      "Classes: 49\n",
      "Training samples: 1,479\n",
      "Test samples: 370\n",
      "Train batches: 46\n",
      "Test batches: 12\n",
      "Mean: ['0.5920', '0.4656', '0.4251']\n",
      "Std: ['0.1773', '0.1639', '0.1615']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Use a very small sample for testing (1% of data) to speed up embedding computation\n",
    "sample_frac = 0.01  # 1% of data for quick testing\n",
    "metadata_gcs_small = (metadata_gcs.groupby('label', group_keys=False)\n",
    "                      .sample(frac=sample_frac, random_state=42)\n",
    "                      .reset_index(drop=True))\n",
    "logger.info(f\"After sampling: {len(metadata_gcs_small):,} samples ({sample_frac*100:.1f}% of {len(metadata_gcs):,} total)\")\n",
    "\n",
    "# Filter out classes with too few samples for stratified splitting\n",
    "# For test_size=0.2, we need at least 5 samples per class (1 in test, 4 in train)\n",
    "min_samples_per_class = 5\n",
    "label_counts = metadata_gcs_small['label'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_samples_per_class].index\n",
    "metadata_gcs_small = metadata_gcs_small[metadata_gcs_small['label'].isin(valid_labels)].reset_index(drop=True)\n",
    "dropped_classes = len(label_counts) - len(valid_labels)\n",
    "logger.info(f\"Filtered out {dropped_classes} classes with < {min_samples_per_class} samples\")\n",
    "logger.info(f\"Final dataset: {len(metadata_gcs_small):,} samples across {len(valid_labels)} classes\")\n",
    "\n",
    "# Compute/load embeddings (REQUIRED for multi-modal model)\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "logger.info(f\"Computing/loading embeddings on device: {device}\")\n",
    "\n",
    "# Embedding configuration (adjust as needed)\n",
    "# Use a local path for small test embeddings to avoid GCS overhead\n",
    "embedding_path = \"embeddings_test_small.parquet\"  # Local file for quick testing\n",
    "embeddings = load_or_compute_embeddings(\n",
    "    data=metadata_gcs_small[[IMG_ID_COL, 'text_desc']],\n",
    "    path=embedding_path,\n",
    "    model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',  # pubmedbert\n",
    "    batch_size=32,\n",
    "    max_length=512,\n",
    "    device=device,\n",
    "    pooling_strategy='mean',\n",
    "    qwen_instr=\"\"\n",
    ")\n",
    "\n",
    "# Merge embeddings with metadata (required for dataloader)\n",
    "metadata_gcs_small = metadata_gcs_small.merge(embeddings, how=\"left\", on=IMG_ID_COL, validate=\"1:1\")\n",
    "logger.info(f\"Embeddings merged. Metadata shape: {metadata_gcs_small.shape}\")\n",
    "\n",
    "# Create config dictionaries for GCS dataloader\n",
    "# Note: create_dataloaders expects data_config, training_config, and augmentation_config\n",
    "data_config = {\n",
    "    'use_local': False, \n",
    "    'img_prefix': '',\n",
    "    'test_size': 0.2,\n",
    "    'val_size': None,\n",
    "    'seed': 42,\n",
    "    'img_size': [224, 224]\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    'batch_size': 32, \n",
    "    'num_workers': 8, \n",
    "    'prefetch_factor': 2,\n",
    "    'compute_stats': True,\n",
    "    'weighted_sampling': True\n",
    "}\n",
    "\n",
    "augmentation_config = {\n",
    "    'brightness_jitter': 0.1,\n",
    "    'contrast_jitter': 0.1,\n",
    "    'saturation_jitter': 0.1,\n",
    "    'hue_jitter': 0.05,\n",
    "    'rotation_degrees': 20,\n",
    "    'translate': [0.1, 0.1],\n",
    "    'scale': [0.9, 1.1],\n",
    "    'grayscale_prob': 0.1,\n",
    "    'horizontal_flip_prob': 0.5,\n",
    "    'vertical_flip_prob': 0.5\n",
    "}\n",
    "\n",
    "# Create dataloaders (works with both GCS and local data)\n",
    "train_loader_gcs, val_loader_gcs, test_loader_gcs, info_gcs = create_dataloaders(\n",
    "    metadata_df=metadata_gcs_small,\n",
    "    img_prefix=img_prefix,  # Uses GCS or local path based on use_local flag\n",
    "    data_config=data_config,\n",
    "    training_config=training_config,\n",
    "    augmentation_config=augmentation_config\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"GCS DataLoader (sampled) Created!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Classes: {info_gcs['num_classes']}\")\n",
    "print(f\"Training samples: {info_gcs['train_size']:,}\")\n",
    "print(f\"Test samples: {info_gcs['test_size']:,}\")\n",
    "print(f\"Train batches: {len(train_loader_gcs)}\")\n",
    "print(f\"Test batches: {len(test_loader_gcs)}\")\n",
    "print(f\"Mean: {[f'{m:.4f}' for m in info_gcs['mean']]}\")\n",
    "print(f\"Std: {[f'{s:.4f}' for s in info_gcs['std']]}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
