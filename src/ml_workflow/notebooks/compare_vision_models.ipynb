{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vision Model Comparison\n",
        "\n",
        "This notebook aggregates and compares results from different vision model experiments:\n",
        "- ResNet-50 (baseline)\n",
        "- EfficientNet-B0\n",
        "- DenseNet-121\n",
        "- ViT-B/16\n",
        "\n",
        "All experiments use the same configuration except for the vision model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Initialize wandb API\n",
        "api = wandb.Api()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wandb configuration\n",
        "WANDB_PROJECT = \"APCOMP215\"\n",
        "\n",
        "# Experiment run names (should match the config filenames)\n",
        "EXPERIMENTS = {\n",
        "    \"ResNet-50\": \"exp_001_baseline\",\n",
        "    \"EfficientNet-B0\": \"exp_002_efficientnet_b0\",\n",
        "    \"DenseNet-121\": \"exp_003_densenet121\",\n",
        "    \"ViT-B/16\": \"exp_004_vit_b16\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_run_data(model_name, run_name):\n",
        "    \"\"\"Fetch the latest run data for a given experiment name\"\"\"\n",
        "    try:\n",
        "        runs = api.runs(WANDB_PROJECT, filters={\"display_name\": run_name}, order=\"-created_at\")\n",
        "        if len(runs) == 0:\n",
        "            print(f\"⚠️  No runs found for {run_name}\")\n",
        "            return None\n",
        "        # Get the most recent run\n",
        "        run = runs[0]\n",
        "        print(f\"✓ Found run: {run.name} ({run.state}) for {model_name}\")\n",
        "        history = run.history()\n",
        "        summary = run.summary\n",
        "        \n",
        "        return {\n",
        "            \"model_name\": model_name,\n",
        "            \"run_name\": run_name,\n",
        "            \"run_id\": run.id,\n",
        "            \"state\": run.state,\n",
        "            \"history\": history,\n",
        "            \"summary\": summary,\n",
        "            \"config\": run.config\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error fetching {run_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fetch all experiment data\n",
        "experiment_data = {}\n",
        "for model_name, run_name in EXPERIMENTS.items():\n",
        "    data = fetch_run_data(model_name, run_name)\n",
        "    if data:\n",
        "        experiment_data[model_name] = data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract final metrics for comparison\n",
        "metrics_to_compare = [\n",
        "    \"val/f1\",  # Macro F1 (main metric)\n",
        "    \"val/acc\",  # Validation accuracy\n",
        "    \"val/loss\",  # Validation loss\n",
        "    \"train/f1\",  # Training macro F1\n",
        "    \"train/acc\",  # Training accuracy\n",
        "    \"train/loss\"  # Training loss\n",
        "]\n",
        "\n",
        "summary_data = []\n",
        "for model_name, data in experiment_data.items():\n",
        "    summary = data[\"summary\"]\n",
        "    row = {\"Model\": model_name}\n",
        "    \n",
        "    for metric in metrics_to_compare:\n",
        "        # Try different metric name variations\n",
        "        metric_key = None\n",
        "        for key in summary.keys():\n",
        "            if metric in key or key.endswith(metric.split(\"/\")[-1]):\n",
        "                metric_key = key\n",
        "                break\n",
        "        \n",
        "        if metric_key and metric_key in summary:\n",
        "            value = summary[metric_key]\n",
        "            # Handle wandb artifacts\n",
        "            if hasattr(value, 'value'):\n",
        "                value = value.value\n",
        "            row[metric] = value\n",
        "        else:\n",
        "            row[metric] = None\n",
        "    \n",
        "    summary_data.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.set_index(\"Model\")\n",
        "\n",
        "# Display summary table\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL METRICS COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(summary_df.round(4))\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics_to_plot = [\n",
        "    (\"val/f1\", \"Validation Macro F1\", \"Higher is better\"),\n",
        "    (\"val/acc\", \"Validation Accuracy (%)\", \"Higher is better\"),\n",
        "    (\"val/loss\", \"Validation Loss\", \"Lower is better\"),\n",
        "    (\"train/f1\", \"Training Macro F1\", \"Higher is better\"),\n",
        "    (\"train/acc\", \"Training Accuracy (%)\", \"Higher is better\"),\n",
        "    (\"train/loss\", \"Training Loss\", \"Lower is better\")\n",
        "]\n",
        "\n",
        "for idx, (metric_key, title, note) in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    for model_name, data in experiment_data.items():\n",
        "        history = data[\"history\"]\n",
        "        \n",
        "        # Find the metric column (handle variations)\n",
        "        metric_col = None\n",
        "        for col in history.columns:\n",
        "            if metric_key in col or col.endswith(metric_key.split(\"/\")[-1]):\n",
        "                metric_col = col\n",
        "                break\n",
        "        \n",
        "        if metric_col and metric_col in history.columns:\n",
        "            epochs = history.get(\"_step\", range(len(history)))\n",
        "            values = history[metric_col]\n",
        "            \n",
        "            # Remove NaN values\n",
        "            mask = ~values.isna()\n",
        "            epochs_clean = epochs[mask] if hasattr(epochs, '__getitem__') else np.array(epochs)[mask]\n",
        "            values_clean = values[mask]\n",
        "            \n",
        "            ax.plot(epochs_clean, values_clean, label=model_name, linewidth=2, alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel(\"Epoch\", fontsize=11)\n",
        "    ax.set_ylabel(title.split(\"(\")[0].strip(), fontsize=11)\n",
        "    ax.set_title(f\"{title}\\n({note})\", fontsize=12, fontweight='bold')\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
